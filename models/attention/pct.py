# https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/conv/point_transformer_conv.html#PointTransformerConv

from typing import Callable, Optional, Tuple, Union

from torch import Tensor

from torch_geometric.nn.conv import MessagePassing
from torch_geometric.nn.dense.linear import Linear
from torch_geometric.nn.inits import reset
from torch_geometric.typing import (
    Adj,
    OptTensor,
    PairTensor,
    SparseTensor,
    torch_sparse,
)
from torch_geometric.utils import add_self_loops, remove_self_loops, softmax


class PCTAttention(MessagePassing):
    def __init__(
        self,
        coords_shape,
        add_self_loops: bool = True,
        **kwargs,
    ):
        kwargs.setdefault("aggr", "add")

        super().__init__()

        self.dim_per_head = kwargs["h_dim"]
        self.num_heads = kwargs["num_heads"]

        self.in_channels = self.dim_per_head * self.num_heads
        self.out_channels = self.dim_per_head
        #self.out_channels = self.dim_per_head * 2
        self.add_self_loops = add_self_loops

        self.pos_nn = Linear(coords_shape, self.out_channels)

        self.attn_nn = Linear(self.out_channels, self.out_channels)
        self.lin = Linear(self.in_channels, self.out_channels, bias=False)
        self.lin_src = Linear(self.in_channels, self.out_channels, bias=False)
        self.lin_dst = Linear(self.in_channels, self.out_channels, bias=False)

        self.reset_parameters()

    def reset_parameters(self):
        super().reset_parameters()
        reset(self.pos_nn)
        if self.attn_nn is not None:
            reset(self.attn_nn)
        self.lin.reset_parameters()
        self.lin_src.reset_parameters()
        self.lin_dst.reset_parameters()

    def forward(self, x, **kwargs) -> Tensor:
        pos = kwargs["coords"]
        edge_index = kwargs["edge_index"]

        if isinstance(x, Tensor):
            alpha = (self.lin_src(x), self.lin_dst(x))
            x: PairTensor = (self.lin(x), x)
        else:
            alpha = (self.lin_src(x[0]), self.lin_dst(x[1]))
            x = (self.lin(x[0]), x[1])

        if isinstance(pos, Tensor):
            pos: PairTensor = (pos, pos)

        if self.add_self_loops:
            if isinstance(edge_index, Tensor):
                edge_index, _ = remove_self_loops(edge_index)
                edge_index, _ = add_self_loops(edge_index, num_nodes=min(pos[0].size(0), pos[1].size(0)))
            elif isinstance(edge_index, SparseTensor):
                edge_index = torch_sparse.set_diag(edge_index)

        # propagate_type: (x: PairTensor, pos: PairTensor, alpha: PairTensor)
        out = self.propagate(edge_index, x=x, pos=pos, alpha=alpha, size=None)
        return out

    def message(
        self,
        x_j: Tensor,
        pos_i: Tensor,
        pos_j: Tensor,
        alpha_i: Tensor,
        alpha_j: Tensor,
        index: Tensor,
        ptr: OptTensor,
        size_i: Optional[int],
    ) -> Tensor:
        delta = self.pos_nn(pos_i - pos_j)
        alpha = alpha_i - alpha_j + delta
        if self.attn_nn is not None:
            alpha = self.attn_nn(alpha)
        alpha = alpha.squeeze()
        
        alpha = softmax(alpha, index, ptr, size_i)
        #alpha = softmax(alpha, index, ptr, size_i if size_i is not None else alpha.size(0))
        return alpha * (x_j + delta)

    def __repr__(self) -> str:
        return f"{self.__class__.__name__}({self.in_channels}, " f"{self.out_channels})"
